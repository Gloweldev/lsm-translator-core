# DocumentaciÃ³n: Sistema de Entrenamiento LSM-Core

## ğŸ“Œ Resumen Ejecutivo

El Transformer **NO aprende imÃ¡genes ni "posiciones visuales"**. Aprende **secuencias de coordenadas numÃ©ricas** que representan movimientos del cuerpo a travÃ©s del tiempo.

---

## ğŸ¯ Â¿QuÃ© Aprende el Modelo?

### Entrada: Vector de 266 nÃºmeros por frame

```
Frame â†’ [x0, y0, x1, y1, x2, y2, ..., x132, y132]
              â† 133 keypoints Ã— 2 coordenadas = 266 â”€â†’
```

### Estructura de los 133 keypoints (RTMPose WholeBody)

| RegiÃ³n | Keypoints | Ãndices | Dimensiones |
|--------|-----------|---------|-------------|
| Cuerpo | 17 puntos | 0-16 | 0-33 |
| Pies | 6 puntos | 17-22 | 34-45 |
| Cara | 68 puntos | 23-90 | 46-181 |
| Mano Izq | 21 puntos | 91-111 | 182-223 |
| Mano Der | 21 puntos | 112-132 | 224-265 |

### Secuencia temporal

```
Video de 90 frames:
â”œâ”€ Frame 1:  [266 coordenadas]
â”œâ”€ Frame 2:  [266 coordenadas]
â”œâ”€ Frame 3:  [266 coordenadas]
...
â””â”€ Frame 90: [266 coordenadas]

Tensor de entrada: (90, 266) â†’ Secuencia Ã— Features
```

---

## ğŸ§  Arquitectura del Modelo

### `LSMTransformer` (transformer.py)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Entrada (90Ã—266) â†’  â”‚ Feature Weights â”‚ â†’ Prioriza manos (Ã—2.5)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Embedding     â”‚ â†’ Proyecta 266 â†’ 512 dims
                    â”‚   + LayerNorm   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Positional Enc. â”‚ â†’ Codifica ORDEN temporal
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Transformer    â”‚ â†’ 4 capas, 8 heads
                    â”‚    Encoder      â”‚ â†’ Aprende patrones
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Clasificador  â”‚ â†’ 512 â†’ 256 â†’ 5 clases
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
               Salida: [prob_a, prob_b, prob_c, prob_hola, prob_nada]
```

---

## âš™ï¸ ConfiguraciÃ³n (settings.py)

### Dimensiones del modelo
```python
INPUT_DIM = 266       # 133 keypoints Ã— 2 (x,y)
D_MODEL = 512         # DimensiÃ³n interna del Transformer
N_HEADS = 8           # Cabezas de atenciÃ³n
N_LAYERS = 4          # Capas del encoder
MAX_SEQ_LEN = 90      # MÃ¡ximo frames por secuencia
```

### Feature Weights (prioridad por regiÃ³n)
```python
FEATURE_WEIGHTS = {
    'body': 1.0,       # Torso normal
    'feet': 0.3,       # Pies menos relevantes
    'face': 0.1,       # Cara casi ignorada
    'left_hand': 2.5,  # Mano izquierda Ã—2.5
    'right_hand': 2.5  # Mano derecha Ã—2.5
}
```

Las manos tienen peso **25Ã— mayor** que la cara. Esto porque las seÃ±as dependen de las manos, no de expresiones faciales.

---

## ğŸ“Š Flujo de Entrenamiento (train.py)

### 1. Carga de datos
```python
# Lee archivos .npy del preprocessing
sequences, labels = load_dataset(PROCESSED_DATA_DIR)
# Cada .npy contiene: array de shape (num_frames, 266)
```

### 2. Dataset con AugmentaciÃ³n
```python
LSMDataset(sequences, labels, augment=True)
```

**Augmentaciones temporales (crÃ­ticas):**

| AugmentaciÃ³n | Probabilidad | PropÃ³sito |
|--------------|--------------|-----------|
| Random Crop | 50% | Corta el video para evitar "bias de retorno al reposo" |
| **FPS Warping** | **50%** | **Simula 15-60 FPS (0.5x-2.0x)** |
| Start Offset | 30% | Simula inicio tardÃ­o del gesto |
| Gaussian Noise | 50% | Robustez a ruido en keypoints |
| Frame Dropout | 20% | Simula frames perdidos |

#### FPS Warping (nuevo en v2.1)
```python
warp_type = choice(['slow', 'normal', 'fast'], p=[0.3, 0.4, 0.3])

slow:   0.5x - 0.7x  # Simula video a 60 FPS
normal: 0.9x - 1.1x  # Velocidad original
fast:   1.5x - 2.0x  # Simula video a 15 FPS (como real-time!)
```

Esto permite que el modelo funcione correctamente en inferencia real-time a 14-15 FPS.

### 3. Balanceo de clases
```python
WeightedRandomSampler(weights, len(labels))
# Muestrea mÃ¡s las clases con menos ejemplos
```

### 4. Training loop
```python
for epoch in range(EPOCHS):
    train_loss, train_acc = train_epoch(...)  # Forward + Backward
    val_loss, val_acc = evaluate(...)         # Solo forward
    
    # Early stopping si no mejora en 10 epochs
    if no_improvement:
        break
```

---

## ğŸ“ Â¿QuÃ© "Entiende" el Transformer?

El modelo aprende **patrones temporales de coordenadas**:

1. **PosiciÃ³n relativa de manos** respecto a caderas
2. **Trayectoria** de movimiento (hacia arriba, circular, etc.)
3. **Velocidad** del gesto
4. **Forma de la mano** (configuraciÃ³n de dedos)
5. **SincronizaciÃ³n** entre mano izquierda y derecha

### Ejemplo conceptual:

```
SeÃ±a "HOLA" â†’ El modelo aprende:
â”œâ”€ Mano cerca de cara al inicio
â”œâ”€ Movimiento lateral ondulatorio
â”œâ”€ Dedos extendidos y separados
â””â”€ Secuencia temporal de ~60 frames
```

---

## ğŸ“ Archivos del sistema

| Archivo | FunciÃ³n |
|---------|---------|
| `settings.py` | ConfiguraciÃ³n global |
| `transformer.py` | Arquitectura del modelo + `forward_with_analysis()` |
| `train.py` | Script de entrenamiento |
| `preprocessor.py` | Extrae keypoints de videos |
| `inspect_processed.py` | Valida dataset + anÃ¡lisis de FPS |
| `ipad_demo.py` | Inferencia real-time con diagnÃ³stico |
| `video_demo.py` | ValidaciÃ³n con videos (soporta .mov, .mp4) |

---

## â“ Preguntas Frecuentes

### Â¿El modelo ve la imagen del video?
**NO.** Solo ve coordenadas numÃ©ricas (x, y) de cada keypoint.

### Â¿Por quÃ© las manos tienen mÃ¡s peso?
Porque las seÃ±as en LSM se definen principalmente por las manos. La cara tiene peso 0.1 porque no aporta a la clasificaciÃ³n.

### Â¿QuÃ© pasa si el FPS varÃ­a?
El modelo fue entrenado con **augmentaciÃ³n de FPS** (0.5x-2.0x) que cubre desde 15 FPS hasta 60 FPS. Esto lo hace robusto a variaciones de velocidad.

### Â¿Por quÃ© Random Crop es tan importante?
Sin Ã©l, el modelo aprende a detectar seÃ±as solo cuando las manos bajan al final del video ("bias de retorno al reposo").

### Â¿CÃ³mo puedo ver quÃ© estÃ¡ "pensando" el modelo?
Usa el **Panel de DiagnÃ³stico** en `ipad_demo.py` (tecla [D]) que muestra:
- Probabilidades de todas las clases
- Importancia por regiÃ³n del cuerpo
- Frame mÃ¡s importante de la secuencia

---

## ğŸ“ˆ MÃ©tricas TÃ­picas

```
Entrenamiento exitoso:
â”œâ”€ Train Accuracy: ~98%
â”œâ”€ Val Accuracy: ~96-97%
â”œâ”€ Epochs: 50-80
â””â”€ Early stopping por patience
```

